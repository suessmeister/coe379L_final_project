{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37966f70-5bf3-4ff6-ba90-cd84a016f821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'coe379L_final_project' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/suessmeister/coe379L_final_project.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755b56e2-8584-413f-8198-54c966c93fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhint: Pulling without specifying how to reconcile divergent branches is\u001b[m\n",
      "\u001b[33mhint: discouraged. You can squelch this message by running one of the following\u001b[m\n",
      "\u001b[33mhint: commands sometime before your next pull:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
      "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
      "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
      "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
      "\u001b[33mhint: invocation.\u001b[m\n",
      "From https://github.com/suessmeister/coe379L_final_project\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!cd coe379L_final_project && git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88958549-a8fa-4d53-81e6-fbb20befde1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/site-packages (from librosa) (0.63.1)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/site-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /usr/local/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.46.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from pooch>=1.1->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d607f3c-47ac-4d87-9e56-bbc4f61b5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "770c6f6a-77d8-4877-8f1d-aa98c2fc8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"hey\", \"hi\", \"hello\"]\n",
    "base_path = \"coe379L_final_project/data\"\n",
    "sample_rate = 16000\n",
    "n_mfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0397bdc-4cc4-463a-8e31-75875362f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_mean(file_path, n_mfcc=n_mfcc, sr=sample_rate):\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60faef51-0205-4d4e-9583-3edca36d804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for label in folders:\n",
    "    folder_path = os.path.join(base_path, label)\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: Folder missing -> {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_mfcc_mean(file_path)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15261aba-59c4-40f6-a720-4d67cce997c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_cat = to_categorical(y_encoded)\n",
    "# Save the encoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b85fb8-ccd7-4e1c-9afb-a5b893d857fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 1s 10ms/step - loss: 5.8993 - accuracy: 0.2167 - val_loss: 3.5931 - val_accuracy: 0.2222\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.8294 - accuracy: 0.3000 - val_loss: 2.1171 - val_accuracy: 0.5185\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.7063 - accuracy: 0.4333 - val_loss: 1.8505 - val_accuracy: 0.4074\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.3226 - accuracy: 0.5833 - val_loss: 0.9169 - val_accuracy: 0.5926\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.7596 - accuracy: 0.7667 - val_loss: 0.9802 - val_accuracy: 0.7037\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5666 - accuracy: 0.7833 - val_loss: 0.4217 - val_accuracy: 0.7407\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.8619 - accuracy: 0.7333 - val_loss: 0.5233 - val_accuracy: 0.8148\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5903 - accuracy: 0.8333 - val_loss: 0.4618 - val_accuracy: 0.7407\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8333 - val_loss: 0.7149 - val_accuracy: 0.6667\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3733 - accuracy: 0.8333 - val_loss: 0.1273 - val_accuracy: 0.9630\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.9000 - val_loss: 0.3081 - val_accuracy: 0.8889\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8333 - val_loss: 0.1883 - val_accuracy: 0.9259\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9167 - val_loss: 0.1311 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2302 - accuracy: 0.8667 - val_loss: 0.1751 - val_accuracy: 0.9630\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.9667 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9667 - val_loss: 0.3025 - val_accuracy: 0.8148\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4203 - accuracy: 0.9000 - val_loss: 0.1603 - val_accuracy: 0.9630\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.9167 - val_loss: 0.3618 - val_accuracy: 0.7778\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9667 - val_loss: 0.1017 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9667 - val_loss: 0.1688 - val_accuracy: 0.9259\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8667 - val_loss: 0.0319 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9667 - val_loss: 0.0562 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9667 - val_loss: 0.1367 - val_accuracy: 0.8889\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.9833 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0672 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7d998c4bc850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_cat, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(n_mfcc,)),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(len(folders), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=4, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6f17a9b-31ac-4d32-8814-d87712449620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0672 - accuracy: 1.0000\n",
      "\n",
      "Final Test Accuracy: 1.0000\n",
      "\n",
      "Saved model as dense_mfcc_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n",
    "model.save(\"artifacts/dense_mfcc_model.h5\")\n",
    "print(\"\\nSaved model as dense_mfcc_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698299b-831d-4ea3-99a6-9a39bf2295f6",
   "metadata": {},
   "source": [
    "## The next step will be to test on an unseen dataset. I will use a different microphone as well as the same microphone and compare MFCC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea8d4ea2-39d1-4a14-813f-ac993d370537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_path):\n",
    "    for label in folders:\n",
    "        folder_path = os.path.join(valid_path, label)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Missing validation folder: {folder_path}\")\n",
    "            continue\n",
    "    \n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "                features = extract_mfcc_mean(file_path)\n",
    "                features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "                pred = model.predict(features)\n",
    "                pred_idx = np.argmax(pred)\n",
    "                predicted_label = le.inverse_transform([pred_idx])[0]\n",
    "    \n",
    "                print(f\"{file} ({label}) -> Predicted: {predicted_label}\")\n",
    "    \n",
    "                y_true.append(label)\n",
    "                y_pred.append(predicted_label)\n",
    "\n",
    "    print(\"\\nCONFUSION MATRIX:\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=folders))\n",
    "    \n",
    "    print(\"\\nCLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_true, y_pred, labels=folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8bb3d7-9201-4675-9403-3baffa9cf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "valid_path_1 = \"coe379L_final_project/valid_data_same_speaker\" # same mic array.\n",
    "valid_path_2 = \"coe379L_final_project/valid_data\" # different mic array with added background noise.  \n",
    "valid_paths = [valid_path_1, valid_path_2]\n",
    "\n",
    "model = load_model(\"dense_mfcc_model.h5\")\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5c1073c-75bc-4432-a603-4d3ff700640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "vh2.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vh3.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vh4.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vh5.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vh1.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vi2.wav (hi) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vi4.wav (hi) -> Predicted: hi\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vi3.wav (hi) -> Predicted: hi\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vi5.wav (hi) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vi1.wav (hi) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vj3.wav (hello) -> Predicted: hi\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vj4.wav (hello) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "vj1.wav (hello) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vj5.wav (hello) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vj2.wav (hello) -> Predicted: hello\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[5 0 0]\n",
      " [1 2 2]\n",
      " [1 1 3]]\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         hey       0.71      1.00      0.83         5\n",
      "          hi       0.67      0.40      0.50         5\n",
      "       hello       0.60      0.60      0.60         5\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.66      0.67      0.64        15\n",
      "weighted avg       0.66      0.67      0.64        15\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vh2.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vh3.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vh4.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vh5.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vh1.wav (hey) -> Predicted: hey\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vi2.wav (hi) -> Predicted: hi\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vi4.wav (hi) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vi3.wav (hi) -> Predicted: hi\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vi5.wav (hi) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vi1.wav (hi) -> Predicted: hi\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vj3.wav (hello) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vj4.wav (hello) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vj1.wav (hello) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "vj5.wav (hello) -> Predicted: hello\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "vj2.wav (hello) -> Predicted: hello\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[10  0  0]\n",
      " [ 1  5  4]\n",
      " [ 1  1  8]]\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         hey       0.83      1.00      0.91        10\n",
      "          hi       0.83      0.50      0.62        10\n",
      "       hello       0.67      0.80      0.73        10\n",
      "\n",
      "    accuracy                           0.77        30\n",
      "   macro avg       0.78      0.77      0.75        30\n",
      "weighted avg       0.78      0.77      0.75        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [] \n",
    "y_pred = []\n",
    "\n",
    "for valid_path in valid_paths:\n",
    "    validate(valid_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd104de0-0e7c-4e7f-9035-c053fe5f31e5",
   "metadata": {},
   "source": [
    "## Now, let's put this on our STM32 chip. We'll export some of the same test samples to demonstrate this workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75c9ab1d-0919-40ec-b1f5-2af252c82b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the training data\n",
    "n_mfcc = 13\n",
    "sample_rate = 16000\n",
    "\n",
    "def get_C_array(file_path):\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    \n",
    "    # Extract MFCC, just like we did in the training\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Average over time \n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    \n",
    "    print(\"MFCC vector (13 floats):\")\n",
    "    print(mfcc_mean)\n",
    "    \n",
    "    # Print in C array format for STM32\n",
    "    print(\"\\nHere is the C array for this sample\")\n",
    "    c_array = \", \".join([f\"{x:.4f}\" for x in mfcc_mean]) # this line was generated by AI\n",
    "    print(f\"float test_mfcc[13] = {{ {c_array} }};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95ff38a5-f010-4bbe-82fb-8e79446a3024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coe379L_final_project/valid_data_same_speaker/hi/vi4.wav\n",
      "MFCC vector (13 floats):\n",
      "[-5.7831769e+02  4.3509212e+01 -1.4505867e+01  5.1716151e+00\n",
      " -1.4796649e+01 -2.1161077e+00 -4.4688196e+00 -4.5387645e+00\n",
      "  1.6780006e+00  3.7075694e+00 -5.5144513e-01 -3.7968655e+00\n",
      " -4.8061695e+00]\n",
      "\n",
      "Here is the C array for this sample\n",
      "float test_mfcc[13] = { -578.3177, 43.5092, -14.5059, 5.1716, -14.7966, -2.1161, -4.4688, -4.5388, 1.6780, 3.7076, -0.5514, -3.7969, -4.8062 };\n",
      "coe379L_final_project/valid_data/hi/vi3.wav\n",
      "MFCC vector (13 floats):\n",
      "[-567.71204    106.95062    -22.197748    -3.5886517    3.8074598\n",
      "   20.865473    18.63842    -13.193054    -7.8210144   10.953506\n",
      "   21.705854     2.9293861   -7.7166734]\n",
      "\n",
      "Here is the C array for this sample\n",
      "float test_mfcc[13] = { -567.7120, 106.9506, -22.1977, -3.5887, 3.8075, 20.8655, 18.6384, -13.1931, -7.8210, 10.9535, 21.7059, 2.9294, -7.7167 };\n",
      "coe379L_final_project/valid_data_same_speaker/hey/vh3.wav\n",
      "MFCC vector (13 floats):\n",
      "[-5.6070874e+02  3.5677265e+01 -8.2513628e+00  2.2864223e+01\n",
      " -4.3454518e+00 -1.2599900e+01 -3.6110904e+00 -4.3537545e+00\n",
      " -2.9299447e-01 -1.8391426e+00 -3.9380748e+00  1.1440738e+00\n",
      " -2.5494516e+00]\n",
      "\n",
      "Here is the C array for this sample\n",
      "float test_mfcc[13] = { -560.7087, 35.6773, -8.2514, 22.8642, -4.3455, -12.5999, -3.6111, -4.3538, -0.2930, -1.8391, -3.9381, 1.1441, -2.5495 };\n",
      "coe379L_final_project/valid_data/hey/vh1.wav\n",
      "MFCC vector (13 floats):\n",
      "[-586.4382      98.09335     -2.986605    26.110512    11.589435\n",
      "    3.5701199   10.050391    -3.7188091   -2.6550922    4.467561\n",
      "   11.379006     4.4829383   -1.194156 ]\n",
      "\n",
      "Here is the C array for this sample\n",
      "float test_mfcc[13] = { -586.4382, 98.0934, -2.9866, 26.1105, 11.5894, 3.5701, 10.0504, -3.7188, -2.6551, 4.4676, 11.3790, 4.4829, -1.1942 };\n",
      "coe379L_final_project/valid_data_same_speaker/hello/vj5.wav\n",
      "MFCC vector (13 floats):\n",
      "[-5.8063812e+02  4.4777756e+01 -3.2233112e+00  1.8850827e+00\n",
      " -9.3100853e+00  1.1831276e-01 -9.1795611e-01 -3.9311588e+00\n",
      " -1.8744315e+00  4.2856257e-02  6.3346934e-01  1.2990043e-01\n",
      " -3.6896746e+00]\n",
      "\n",
      "Here is the C array for this sample\n",
      "float test_mfcc[13] = { -580.6381, 44.7778, -3.2233, 1.8851, -9.3101, 0.1183, -0.9180, -3.9312, -1.8744, 0.0429, 0.6335, 0.1299, -3.6897 };\n",
      "coe379L_final_project/valid_data/hello/vj4.wav\n",
      "MFCC vector (13 floats):\n",
      "[-566.2546     119.804405    -3.468045    -0.710839     1.0563204\n",
      "   16.547445    19.086666   -12.654387   -11.952149    10.42644\n",
      "   17.468672     5.273083    -6.344156 ]\n",
      "\n",
      "Here is the C array for this sample\n",
      "float test_mfcc[13] = { -566.2546, 119.8044, -3.4680, -0.7108, 1.0563, 16.5474, 19.0867, -12.6544, -11.9521, 10.4264, 17.4687, 5.2731, -6.3442 };\n"
     ]
    }
   ],
   "source": [
    "hi1 = \"coe379L_final_project/valid_data_same_speaker/hi/vi4.wav\" # hi examples\n",
    "hi2 = \"coe379L_final_project/valid_data/hi/vi3.wav\" \n",
    "\n",
    "hey1 = \"coe379L_final_project/valid_data_same_speaker/hey/vh3.wav\" # hey examples\n",
    "hey2 = \"coe379L_final_project/valid_data/hey/vh1.wav\" \n",
    "\n",
    "hello1 = \"coe379L_final_project/valid_data_same_speaker/hello/vj5.wav\" # hello examples\n",
    "hello2 = \"coe379L_final_project/valid_data/hello/vj4.wav\" \n",
    "\n",
    "paths = [hi1, hi2, hey1, hey2, hello1, hello2]\n",
    "\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    get_C_array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc49904-7065-47a1-ae8c-cb87aa83f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"hey\", \"hi\", \"hello\"]\n",
    "n_mfcc = 13\n",
    "sample_rate = 16000\n",
    "\n",
    "valid_path = \"coe379L_final_project/valid_data\"\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=n_mfcc, sr=sample_rate):\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "# Loop through each class folder\n",
    "for label in folders:\n",
    "    folder_path = os.path.join(valid_path, label)\n",
    "    if not os.path.exists(folder_path):\n",
    "        continue\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            mfcc_features = extract_mfcc(file_path)\n",
    "            prediction = model.predict(np.expand_dims(mfcc_features, axis=0))\n",
    "            predicted_label = folders[np.argmax(prediction)]\n",
    "            print(f\"{file} ({label}) -> Predicted: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f356e9-82aa-47ae-90ca-b9644c7f116a",
   "metadata": {},
   "source": [
    "## Alternative Implementation -- Not Working, But an Interesting NN approach nonetheless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ce2ea-cafb-4ef6-86a7-92ac6686d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "folders = [\"hey\", \"hi\", \"hello\"]\n",
    "base_path = \"coe379L_final_project/data\"  # path to training data\n",
    "sample_rate = 16000\n",
    "n_mfcc = 13  \n",
    "max_len = 32  # alternative approach -- do not do past 32 MFCC \n",
    "\n",
    "# an alternative approach to extracting MFCC based on fixed length \n",
    "def extract_mfcc(file_path, n_mfcc=n_mfcc, sr=sample_rate, max_len=max_len):\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0,0),(0,pad_width)), mode='constant') # This line was generated by AI\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for label in folders:\n",
    "    folder_path = os.path.join(base_path, label)\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            mfcc = extract_mfcc(file_path)\n",
    "            X.append(mfcc)\n",
    "            y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "X = X[..., np.newaxis]  # This line was generated by AI \n",
    "\n",
    "# encoding \n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "\n",
    "n_samples, n_mfcc, n_time, n_channel = X.shape\n",
    "X_reshaped = X.reshape((n_samples, n_mfcc*n_time)) # This line was generated by AI \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X = X_scaled.reshape((n_samples, n_mfcc, n_time, n_channel))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3,3), activation='relu', input_shape=(n_mfcc, max_len, 1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(folders), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=4, validation_data=(X_test, y_test))\n",
    "\n",
    "# predictions now \n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "525578d0-5f8f-4d89-b320-06567212b7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "Raw model outputs: [[0.2952013  0.04858567 0.6562131 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load model and label encoder\n",
    "model = load_model(\"dense_mfcc_model.h5\")\n",
    "# with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "#     le = pickle.load(f)\n",
    "\n",
    "hi1_array = np.array([\n",
    "    -578.3177, 43.5092, -14.5059, 5.1716, -14.7966, -2.1161, -4.4688,\n",
    "    -4.5388, 1.6780, 3.7076, -0.5514, -3.7969, -4.8062\n",
    "], dtype=np.float32)\n",
    "\n",
    "hi1_array = hi1_array.reshape(1, -1)\n",
    "pred = model.predict(hi1_array)\n",
    "print(\"Raw model outputs:\", pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d992dfe-0b37-46c8-bb55-84959f9f29ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
